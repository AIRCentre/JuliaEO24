{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/JuliaEO2024_banner.png\" alt=\"Descriptive Image Text\" style=\"border:1px solid black\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "The objectives of this workshop are the following:\n",
    "\n",
    "\n",
    "- **Understand SAR Imagery**: Gain an understanding of SAR imagery, its properties, and why it's advantageous for maritime surveillance.\n",
    "- **Data Handling**: Learn how to acquire and prepare SAR data from the Sentinel-1 satellite.\n",
    "- **Apply Machine Learning**: Explore machine learning techniques, from basic classification to object detection models.\n",
    "- **Real-World Application**: Gain practical skills in applying these models to real-world SAR data for effective maritime surveillance.\n",
    "\n",
    "\n",
    "\n",
    "____________________\n",
    "\n",
    "\n",
    "\n",
    "## Notebooks\n",
    "\n",
    "- 1: [Sentinel-1 data acquisition](1_Data_acqusition.ipynb)\n",
    "- 2: [Ship binary classification](2_Ship_binary_classification.ipynb)\n",
    "- 3: [Two stage ship detection](3_two_stage_ship_detector.ipynb)\n",
    "- 4: [One stage ship detection](4_One_stage_ship_detector.ipynb)\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/overview_notebooks.png\" alt=\"Descriptive Image Text\" width=\"800\" />\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# One stage object detector\n",
    " <a class=\"anchor\" id=\"toc\"></a>\n",
    " \n",
    "-  [Objectives](#Objectives)\n",
    "-  [One stage ship detector](#one_stage_object)\n",
    "-  [Include modules](#init)\n",
    "-  [Data](#data)\n",
    "-  [Transformations](#Transformations)\n",
    "-  [Inferrence](#inferrence)\n",
    "-  [Final remarks](#remarks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Objectives\n",
    " <a class=\"anchor\" id=\"Objectives\"></a>\n",
    "Back to [Table of Content](#toc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is the following:\n",
    "1. Detect ship in a new Sentinel-1 image using a one-stage ship detector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## One stage ship detector\n",
    " <a class=\"anchor\" id=\"one_stage_object\"></a>\n",
    "Back to [Table of Content](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "One-stage object detectors provide a swift and efficient approach for object detection, markedly different from traditional convolutional neural network (CNN) methods. These detectors are designed to perform both object localization and classification in a single, unified step, which is particularly advantageous for real-time applications.\n",
    "\n",
    "\n",
    "\n",
    "- **Unified Detection Framework**: Utilizes a singular neural network that directly predicts bounding boxes and class probabilities from full images in a single analysis, greatly accelerating the detection process.\n",
    "- **Grid-based Detection**: The image is dissected into a grid, with each cell tasked with detecting objects within its area.\n",
    "- **Bounding Box Prediction**: Each grid cell predicts multiple bounding boxes, each accompanied by a confidence score that indicates the probability of an object's presence, along with its class probabilities.\n",
    "\n",
    "\n",
    "\n",
    "Tailoring this architecture for maritime surveillance allows for the specific identification of maritime entities, such as ships.\n",
    "\n",
    "\n",
    "1. **Dataset Preparation**: Assemble a dataset of maritime images, with each significant object (like ships) annotated with a bounding box and class label.\n",
    "2. **Simultaneous Feature Extraction and Prediction**: The model processes the entire image in one go, extracting features while predicting bounding boxes and class probabilities for each grid cell.\n",
    "3. **Post-processing**: Apply advanced techniques like non-maximum suppression to refine bounding box predictions, ensuring precise and distinct object detection.\n",
    "\n",
    "**Advantages for Maritime Surveillance** \n",
    "\n",
    "- **Real-Time Processing**: The speed of this method is vital in the dynamic maritime environment.\n",
    "- **Enhanced Detection Accuracy**: These models are adept at detecting a variety of objects, including small or partially obscured maritime items.\n",
    "- **Flexibility and Adaptability**: With the ability to detect a wide range of objects under different conditions, this method is highly adaptable for various maritime surveillance needs.\n",
    "\n",
    "Implementing a one-stage detector in maritime surveillance significantly enhances monitoring system capabilities, offering rapid, accurate, and efficient real-time object detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Aspect | One-Stage Ship Detector | Two-Stage Ship Detector (CA-CFAR + Binary Classifier) |\n",
    "| ------ | ----------------------- | ---------------------------------------------------- |\n",
    "| **Speed and Efficiency** | High efficiency and speed, suitable for real-time applications. | Generally slower due to the two-stage process, particularly in the CA-CFAR stage. |\n",
    "| **Detection Accuracy** | High accuracy, especially in clear conditions; may struggle with small or overlapping objects. | High accuracy in detection; CA-CFAR excels in distinguishing ships from background noise. |\n",
    "| **Complexity** | Lower complexity with a single unified model. | Higher complexity due to the combination of two distinct stages. Very difficult to optimize |\n",
    "| **Real-Time Capability** | Excellent for real-time detection due to processing speed. | Limited in real-time applications due to the sequential processing stages. |\n",
    "| **Flexibility** | Highly adaptable to various object sizes and conditions. | Adaptability can be limited; effectiveness of object detection depends on the accuracy of the CA-CFAR stage. Effectives of the classifier is HIGHLY dependant on the data and model (especially using softmax loss function). I.e., if the classifier sees a new type of object, it might say there is a 100 % it is a ship, while infact it is not. |\n",
    "| **Resource Intensity** | Less resource-intensive as it involves a single neural network. | More resource-intensive, requiring computational power for both detection stages. |\n",
    "| **Robustness to Noise** | Robust depending on training method. | The CA-CFAR stage is specifically designed to handle high clutter and noise levels. |\n",
    "| **Ease of Training** | Complex to develop the model. But simplified training process with a single network. | Needing fine-tuning for both stages. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**One-stage ship detector steps**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, The image is divided into a grid of cells, with each cell tasked with detecting objects within its area.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/notebooks/onestage_1.png\" alt=\"Not a ship\" style=\"width: 500px; border: 1px solid black; display: inline-block;\"/>\n",
    "    <figcaption>Iceye image from the port of Rotterdam. The image is divided into a grid of cells. </figcaption>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "Each cell in the grid predicts bounding boxes, each accompanied by a confidence score that indicates the probability of an object's presence, along with its class probabilities.  (here, red is ship and yellow is crane)\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/notebooks/onestage_2.png\" alt=\"Ship\" style=\"width: 500px; border: 1px solid black; display: inline-block;\"/>\n",
    "    <figcaption>In each cell, different objects will be detected, here illustrated with different colours with  red colours corrsponding to ships. </figcaption>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "For each predicted object, we can illustrate its bounding box (from its estimated center coordinates and width/height). The same object can thus have several bounding boxes. \n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/notebooks/onestage_3.png\" alt=\"Not a ship\" style=\"width: 500px; border: 1px solid black; display: inline-block;\"/>\n",
    "    <figcaption>Each detected object has a corrsponding estimated bounding box. </figcaption>\n",
    "</div>\n",
    "\n",
    "We can then remove the \"extra\" bounding boxes for each object, resulting in one bounding box for each object.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/notebooks/onestage_4.png\" alt=\"Ship\" style=\"width: 500px; border: 1px solid black; display: inline-block;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "The model processes the entire image in one go, extracting features while predicting bounding boxes and class probabilities for each grid cell. Finally, advanced techniques like non-maximum suppression are applied to refine bounding box predictions, ensuring precise and distinct object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model we will be working with today is a confidence score between 0 and 1. The total confidence is a combination of the probability of the object being a ship(class confidence) and the probability of the bounding box being correct (bbox confidence).\n",
    "\n",
    "\n",
    "Box confidence evaluates the likelihood that a bounding box accurately encapsulates an object of interest.  On the other hand, class confidence quantifies the model's confidence in assigning the detected object to a specific category. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For SAR images, it is notoiously difficult to estimate the correct bounding box for a ship, and the confidence score will therefore often be lowered. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When annotating data for such a model, you often label the bounding boxes of your objects. Below, we see and example where ships are annotated as yellow bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/notebooks/bbox_labels.png\" alt=\"Ship\" style=\"width: 1000px; display: inline-block;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of such a bounding box is illustrated below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/notebooks/bbox_zoom.png\" alt=\"Ship\" style=\"width: 600px; display: inline-block;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/notebooks/Onestage_part0.png\" alt=\"Ship\" style=\"width: 1300px; display: inline-block;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Init <a class=\"anchor\" id=\"init\">​</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to [Table of Content](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Pkg\n",
    "#Pkg.add(url=\"https://github.com/aalling93/VesselDetection.jl\")\n",
    "#Pkg.add([\"Makie\",\"GLMakie\",\"Plots\", \"Images\"])\n",
    "using VesselDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg, Makie, GLMakie, Plots\n",
    "GLMakie.activate!(inline=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data \n",
    "<a class=\"anchor\" id=\"data\">​</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to [Table of Content](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the methodology described in [Sentinel-1 data acquisition](1_Data_acqusition.ipynb), we downloaded a Sentinel-1 Image. \n",
    "\n",
    "We now want to detect ships in the same image. For this workshop, a template image is provided in the Datafolder\n",
    "\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/notebooks/Onestage_part1.png\" alt=\"Ship\" style=\"width: 1300px; display: inline-block;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sentinel-1 image can be loaded using the Sentinel1GRD function\n",
    "\n",
    "```julia\n",
    "test_img = VesselDetection.Sensors.Sentinel1.Sentinel1GRD(safe_path);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Task: </b>\n",
    "\n",
    "1. Load the Sentinel-1 IW GRD image from Notebook 1 : S1A_IW_GRDH_1SDV_20220612T173329_20220612T173354_043633_05359A_EA25\n",
    "\n",
    "2. Confirm the image is loaded correctly by plotting the image and the metadata, e.g., by checking the image mode and elevation angle\n",
    "    - println(test_img.metadata.header.swath)\n",
    "    - println(\"Elevation angles: $(test_img.metadata.geolocation.elevation_angle[1:10])\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the image:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_folder = \"../data/S1A_IW_GRDH_1SDV_20220612T173329_20220612T173354_043633_05359A_EA25_test.SAFE\";\n",
    "test_img = VesselDetection.Sensors.Sentinel1.Sentinel1GRD(safe_folder);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = Figure(size = (600, 400))\n",
    "ax = Axis(fig[1, 1], title = \"Image\")\n",
    "im = image!(ax, test_img.data[1][:,:], colormap = :greys)\n",
    "Colorbar(fig[1, 2], im, label = \"Value\", vertical = true)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_matrix = vec(test_img.data[1])\n",
    "histogram(flat_matrix, bins=50, xlabel=\"Value\", ylabel=\"Frequency\", title=\"Histogram of image values\", legend=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(test_img.metadata.header.swath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "<a class=\"anchor\" id=\"Transformations\">​</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of transformations can be applied to the training images for a deep learning model. After a model has been trained, it is important to apply the same transformation to the images that we want to use for inferrence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the TransformCompose, we can apply different transformations to our image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_transform = VesselDetection.Ship_detector.Transformation.TransformCompose([\n",
    "    VesselDetection.Ship_detector.Transformation.absolute,\n",
    "    x -> VesselDetection.Ship_detector.Transformation.clip_to_valid_range(x, amin=1, amax=65535),\n",
    "    VesselDetection.Ship_detector.Transformation.to_db,\n",
    "    x -> VesselDetection.Ship_detector.Transformation.min_max_scale(x, data_max=48.17, data_min=0, to_max=1, to_min=0),\n",
    "    x -> VesselDetection.Ship_detector.Transformation.to_channels(x, [1, 2, 2]),\n",
    "    VesselDetection.Ship_detector.Transformation.to_float32,\n",
    "]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now applying the transformation to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img2 = s1_transform(test_img.data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Task: </b>\n",
    "\n",
    "1. Confirm that the transformations have been applied correctly by plotting the image again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = Figure(size = (600, 400))\n",
    "ax = Axis(fig[1, 1], title = \"Image with applied transforms\")\n",
    "im = image!(ax, test_img2[1,:,:], colormap = :greys, )\n",
    "Colorbar(fig[1, 2], im, label = \"Value\", vertical = true)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_matrix = vec(test_img2[1,:,:])\n",
    "histogram(flat_matrix, bins=50, xlabel=\"Value\", ylabel=\"Frequency\", title=\"Histogram of image values\", legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Inferrence  <a class=\"anchor\" id=\"inferrence\">​</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to [Table of Content](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple one-stage ship detector has been trained on a small dataset. The model is available in the data/models folder. Ofte, models are as large as 500 mb+. The one used in this workshop is  7.8 mb. While its performance is not as good as the state-of-the-art models, it is still able to detect ships in the image very fast.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/notebooks/Onestage_part2.png\" alt=\"Ship\" style=\"width: 1300px; display: inline-block;\"/>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../data/Model/one_stage_model.onnx\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  VesselDetection.Ship_detector.Model.OneStageDetector.load_model(model_path = model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use https://netron.app/ to illustrate the model if you want to look into the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will focus on a small subset of the image containing ships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/notebooks/Onestage_part3.png\" alt=\"Ship\" style=\"width: 1300px; display: inline-block;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = VesselDetection.Ship_detector.Model.OneStageDetector.process_image_for_onnx(model, test_img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/notebooks/Onestage_part4.png\" alt=\"Ship\" style=\"width: 1300px; display: inline-block;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Task: </b>\n",
    "\n",
    "1. Display the detected ships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VesselDetection.Visualize.plot_image_with_boxes(image_matrix = test_img2[2,:,:], bbox_dict = results, confidence_threshold = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Task: </b>\n",
    "\n",
    "We have now detected ship in the image. However, we are not done yet. We should implement a Non max supression, i.e., to supress the \"double\" detections.\n",
    "\n",
    "\n",
    "1. Implement an algorithm to supress the extra detections. \n",
    "\n",
    "2. Implement a method to handle batches of images. \n",
    "\n",
    "3. combine the results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Final remarks:\n",
    "<a class=\"anchor\" id=\"remarks\">​</a>\n",
    "Back to [Table of Content](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ship detection**:\n",
    "\n",
    "In these notebooks we have implemented to different methods for detecting and classifing vessels in SAR imagery:\n",
    "- Two-stage ship detector with a CA-CFAR and binary classifier.\n",
    "- One-stage ship detector trained only to detect ships.\n",
    "\n",
    "While these methods works, they are considered as simple models. In fact, in order to develop better models, you use auxillary information in your models such at the datatime and location, i.e., there is a higher probability of detecting icebergs in wintertime near greenland than summertime near the azores. Furhtermore, depending on your objective you might apply furhter pre-analysis, processing steps etc.\n",
    "\n",
    "If you want to use either the two-stage detector develop here or a one-stage object detector you need to add way more training data. Both models have been trained on very small, non-diverse dataset as proof of concept. \n",
    "\n",
    "**Next steps**:\n",
    "\n",
    "After you have detected a ship, no matter from what data source, you normally perform more analyses. \n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../figures/notebooks/ship.png\" alt=\"Ship\" style=\"width: 400px; border: 1px solid black; display: inline-block;\"/>\n",
    "</div>\n",
    "\n",
    "* What is the size of the ship (length, width)?\n",
    "* What is the direction (sometimes, not always, there is an ambigutiy of 180 degrees)?\n",
    "* What type of ship is it (Fishing, cargo, navy, ....)?\n",
    "* What is the speed of the vessel?\n",
    "* What ship is it (not only type, but what specific vessel is it)?\n",
    "* Is it performing illegal activities?\n",
    "* What is its future trajectory -> where will it be 3 hours in the future?\n",
    "* Does it carry instruments for warfare? An anti-missile system, if yes, with what charecteristics? \n",
    "\n",
    "And many other furhter analysis. However, in all cases, the first step is to detect the vessel, as we did here. \n",
    "We will not dwell furhter into the extra analyses in this workship. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
